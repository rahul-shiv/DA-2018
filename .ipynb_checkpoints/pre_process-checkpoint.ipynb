{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prajwalkirankumar/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2698: DtypeWarning:\n",
      "\n",
      "Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of keywords in variable 'keywords': 11462\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  Searched in:\n    - '/Users/prajwalkirankumar/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/prajwalkirankumar/anaconda3/nltk_data'\n    - '/Users/prajwalkirankumar/anaconda3/share/nltk_data'\n    - '/Users/prajwalkirankumar/anaconda3/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  Searched in:\n    - '/Users/prajwalkirankumar/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/prajwalkirankumar/anaconda3/nltk_data'\n    - '/Users/prajwalkirankumar/anaconda3/share/nltk_data'\n    - '/Users/prajwalkirankumar/anaconda3/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-de15a16656a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_apparitions\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyword_occurences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnb_apparitions\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0mlemma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_synonyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     word_list = [(s, key_count[s]) for s in lemma \n",
      "\u001b[0;32m<ipython-input-14-de15a16656a5>\u001b[0m in \u001b[0;36mget_synonyms\u001b[0;34m(word_key)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_synonyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0mlemma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  Searched in:\n    - '/Users/prajwalkirankumar/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/prajwalkirankumar/anaconda3/nltk_data'\n    - '/Users/prajwalkirankumar/anaconda3/share/nltk_data'\n    - '/Users/prajwalkirankumar/anaconda3/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import plotly.offline as off\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "PS = nltk.stem.PorterStemmer()\n",
    "\n",
    "off.init_notebook_mode(connected=True)\n",
    "keywords = pd.read_csv('keywords.csv')\n",
    "meta = pd.read_csv('movies_metadata.csv')\n",
    "ratings = pd.read_csv('ratings_small.csv')\n",
    "# credit = pd.read_csv('credits.csv')\n",
    "\n",
    "ratings = ratings.rename(columns={'movieId':'id'})\n",
    "#Convert IDs to numeric from string\n",
    "meta = meta[meta.status==\"Released\"]\n",
    "meta = meta[meta.revenue!=0]\n",
    "meta = meta[meta.budget!=0]\n",
    "meta.id = pd.to_numeric(meta.id,errors=\"coerce\")\n",
    "meta = meta.dropna(subset=[\"id\"])\n",
    "meta.release_date=pd.to_datetime(meta.release_date, format = '%Y-%m-%d', errors=\"coerce\")\n",
    "meta = meta.dropna(subset=[\"release_date\"])\n",
    "meta = meta.drop_duplicates(subset=['id'])\n",
    "meta = meta.drop(['homepage','poster_path','production_countries','video','spoken_languages','original_title'], axis =1)\n",
    "keywords = keywords[keywords.keywords!=\"[]\"]\n",
    "keywords = keywords.drop_duplicates(subset=['id'])\n",
    "meta = pd.merge(meta,keywords,on='id',how='left')\n",
    "\n",
    "tab_info=pd.DataFrame(meta.dtypes).T.rename(index={0:'column type'})\n",
    "tab_info=tab_info.append(pd.DataFrame(meta.isnull().sum()).T.rename(index={0:'null values'}))\n",
    "tab_info=tab_info.append(pd.DataFrame(meta.isnull().sum()/meta.shape[0]*100).T.rename(index={0:'null values (%)'}))\n",
    "\n",
    "\n",
    "meta.popularity = pd.to_numeric(meta.popularity, errors='coerce')\n",
    "tasks = ['genres','production_companies','keywords']\n",
    "\n",
    "def count_word(df, ref_col, liste):\n",
    "    keyword_count = dict()\n",
    "    for s in liste: keyword_count[s] = 0\n",
    "    for liste_keywords in df[ref_col]:        \n",
    "        if type(liste_keywords) == float and pd.isnull(liste_keywords): continue        \n",
    "        for s in [s for s in liste_keywords if s in liste]: \n",
    "            if pd.notnull(s): keyword_count[s] += 1\n",
    "    # convert the dictionary in a list to sort the keywords by frequency\n",
    "    keyword_occurences = []\n",
    "    for k,v in keyword_count.items():\n",
    "        keyword_occurences.append([k,v])\n",
    "    keyword_occurences.sort(key = lambda x:x[1], reverse = True)\n",
    "    return keyword_occurences, keyword_count\n",
    "\n",
    "def convert(task):\n",
    "    temp_dict = dict()\n",
    "    temp_arr = []\n",
    "    temp_series = meta[task]\n",
    "    for item in temp_series:\n",
    "        if type(item) != type(str()):\n",
    "            continue\n",
    "        t = literal_eval(item)\n",
    "        if type(t)!=type([]):\n",
    "            continue\n",
    "        temp =[]\n",
    "        for i in t:\n",
    "            temp.append(i['name'])\n",
    "            if str(i['name']) not in temp_dict.keys():\n",
    "                temp_dict[str(i['name'])] = 1\n",
    "            else:\n",
    "                temp_dict[str(i['name'])]+=1\n",
    "        temp_arr.append(temp)\n",
    "    meta[task]=pd.Series(temp_arr)\n",
    "    return temp_dict\n",
    "\n",
    "genre_dict = convert(tasks[0])\n",
    "prodcomp_dict = convert(tasks[1])\n",
    "convert(tasks[2])\n",
    "\n",
    "df_2 = meta\n",
    "def keywords_inventory(dataframe, column = 'keywords'):\n",
    "    keywords_roots  = dict()  # collect the words / root\n",
    "    keywords_select = dict()  # association: root <-> keyword\n",
    "    category_keys = []\n",
    "    icount = 0\n",
    "    for s in dataframe[column]:\n",
    "        if type(s) == type(0.0) : continue\n",
    "        for t in s:\n",
    "            t = t.lower() \n",
    "            root = PS.stem(t)\n",
    "            if root in keywords_roots:                \n",
    "                keywords_roots[root].add(t)\n",
    "            else:\n",
    "                keywords_roots[root] = {t}\n",
    "    for s in keywords_roots.keys():\n",
    "        if len(keywords_roots[s]) > 1:  \n",
    "            min_length = 1000\n",
    "            for k in keywords_roots[s]:\n",
    "                if len(k) < min_length:\n",
    "                    key = k ; min_length = len(k)            \n",
    "            category_keys.append(key)\n",
    "            keywords_select[s] = key\n",
    "        else:\n",
    "            category_keys.append(list(keywords_roots[s])[0])\n",
    "            keywords_select[s] = list(keywords_roots[s])[0]\n",
    "                   \n",
    "    print(\"No of keywords in variable '{}': {}\".format(column,len(category_keys)))\n",
    "    return category_keys, keywords_roots, keywords_select\n",
    "\n",
    "keywords_dat, keywords_roots, keywords_select = keywords_inventory(df_2 ,column = 'keywords')\n",
    "\n",
    "def replacement_df_keywords(df, dict_repl, roots = False):\n",
    "    df_new = df.copy(deep = True)\n",
    "    for index, row in df_new.iterrows():\n",
    "        chain = row['keywords']\n",
    "        if type(chain) == type(0.0) : continue\n",
    "        temp = []\n",
    "        for s in chain: \n",
    "            key = PS.stem(s) if roots else s\n",
    "            if key in dict_repl.keys():\n",
    "                temp.append(dict_repl[key])\n",
    "            else:\n",
    "                temp.append(s)       \n",
    "        df_new.set_value(index, 'keywords', temp)\n",
    "    return df_new\n",
    "\n",
    "df_keywords_cleaned = replacement_df_keywords(df_2, keywords_select,roots = True)\n",
    "\n",
    "keyword_occurences, keywords_count = count_word(df_keywords_cleaned,'keywords',keywords_dat)\n",
    "\n",
    "\n",
    "def get_synonyms(word_key):\n",
    "    lemma = set()\n",
    "    for ss in wordnet.synsets(word_key):\n",
    "        for w in ss.lemma_names():\n",
    "            index = ss.name().find('.')+1\n",
    "            if ss.name()[index] == 'n': lemma.add(w.lower().replace('_',' '))\n",
    "    return lemma   \n",
    "\n",
    "def test_keyword(word, key_count, threshold):\n",
    "    return (False , True)[key_count.get(word, 0) >= threshold]\n",
    "\n",
    "keyword_occurences.sort(key = lambda x:x[1], reverse = False)\n",
    "key_count = dict()\n",
    "for s in keyword_occurences:\n",
    "    key_count[s[0]] = s[1]\n",
    "\n",
    "repl_word = dict()\n",
    "icount = 0\n",
    "for index, [word, nb_apparitions] in enumerate(keyword_occurences):\n",
    "    if nb_apparitions > 5: continue  \n",
    "    lemma = get_synonyms(word)\n",
    "    if len(lemma) == 0: continue  \n",
    "    word_list = [(s, key_count[s]) for s in lemma \n",
    "                  if test_keyword(s, key_count, key_count[word])]\n",
    "    word_list.sort(key = lambda x:(x[1],x[0]), reverse = True)    \n",
    "    if len(word_list) <= 1: continue     \n",
    "    if word == word_list[0][0]: continue\n",
    "    repl_word[word] = word_list[0][0]\n",
    "\n",
    "for key, value in repl_word.items():\n",
    "    if value in repl_word.keys():\n",
    "        repl_word[key] = repl_word[value]    \n",
    "        \n",
    "df_keywords_synonyms = replacement_df_keywords(df_keywords_cleaned, repl_word, roots = False)   \n",
    "\n",
    "keywords_dat, keywords_roots, keywords_select = keywords_inventory(df_keywords_synonyms, column = 'keywords')\n",
    "\n",
    "new_keyword_occurences, keywords_count = count_word(df_keywords_synonyms,\n",
    "                                                    'keywords',keywords_dat)\n",
    "\n",
    "movies_list = []\n",
    "def genre_transform():\n",
    "    temp_dict = dict()\n",
    "    for index,row in meta.iterrows():\n",
    "        movies_list.append(tuple([row.release_date.year,row.revenue]))\n",
    "        for i in row.genres:\n",
    "            if str(i) not in temp_dict.keys():\n",
    "                temp_dict[str(i)] = [tuple([row.release_date.year,row.revenue])]\n",
    "            else:\n",
    "                temp_dict[str(i)].append(tuple([row.release_date.year,row.revenue]))\n",
    "    return temp_dict\n",
    "genre_plot_dict = genre_transform()\n",
    "\n",
    "for key,value in genre_plot_dict.items():\n",
    "    x,y = zip(*sorted(value))\n",
    "    data = [dict(\n",
    "      type = 'scatter',\n",
    "      x = x,\n",
    "      y = y,\n",
    "      transforms = [dict(\n",
    "        type = 'aggregate',\n",
    "        groups = x,\n",
    "        aggregations = [dict(\n",
    "            target = 'y', func = 'sum', enabled = True),\n",
    "        ]\n",
    "      )]\n",
    "    )]\n",
    "    layout = dict(\n",
    "        title='Genre: '+key,\n",
    "        xaxis=dict(\n",
    "            rangeselector=dict(\n",
    "                buttons=list([\n",
    "                    dict(count=1,\n",
    "                         label='1m',\n",
    "                         step='month',\n",
    "                         stepmode='backward'),\n",
    "                    dict(count=6,\n",
    "                         label='6m',\n",
    "                         step='month',\n",
    "                         stepmode='backward'),\n",
    "                    dict(step='all')\n",
    "                ])\n",
    "            ),\n",
    "            rangeslider=dict(\n",
    "                visible = True\n",
    "            ),\n",
    "            type='date',\n",
    "            title = 'Year'\n",
    "        ),\n",
    "        yaxis = dict(title='revenue')\n",
    "            \n",
    "    )\n",
    "    #change to iplot in jupyter notebook\n",
    "    off.iplot({\n",
    "        \"data\": data,\n",
    "        \"layout\": layout\n",
    "    }, validate= False)\n",
    "    \n",
    "x,y = zip(*sorted(movies_list))\n",
    "data = [dict(\n",
    "      type = 'scatter',\n",
    "      x = x,\n",
    "      y = y,\n",
    "      transforms = [dict(\n",
    "        type = 'aggregate',\n",
    "        groups = x,\n",
    "        aggregations = [dict(\n",
    "            target = 'y', func = 'count', enabled = True),\n",
    "        ]\n",
    "      )]\n",
    "    )]\n",
    "layout = dict(\n",
    "    title='Number of movies released across the years',\n",
    "    xaxis=dict(\n",
    "        rangeselector=dict(\n",
    "            buttons=list([\n",
    "                dict(count=1,\n",
    "                     label='1m',\n",
    "                     step='month',\n",
    "                     stepmode='backward'),\n",
    "                dict(count=6,\n",
    "                     label='6m',\n",
    "                     step='month',\n",
    "                     stepmode='backward'),\n",
    "                dict(step='all')\n",
    "            ])\n",
    "        ),\n",
    "        rangeslider=dict(\n",
    "            visible = True\n",
    "        ),\n",
    "        type='date',\n",
    "        title = 'Year'\n",
    "    ),\n",
    "    yaxis = dict(title='number of movies')\n",
    "        \n",
    ")\n",
    "#change to iplot in jupyter notebook\n",
    "off.plot({\n",
    "    \"data\": data,\n",
    "    \"layout\": layout\n",
    "},auto_open=True, validate= False)\n",
    "\n",
    "\n",
    "def convert_key():\n",
    "    temp = \"\"\n",
    "    temp_series = df_keywords_synonyms['keywords']\n",
    "    for item in temp_series:\n",
    "        if type(item)!=type([]):\n",
    "            continue\n",
    "        for i in item:\n",
    "            temp = temp + i\n",
    "    return temp\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "\n",
    "# Creating word cloud for title:\n",
    "meta['title'] = meta['title'].astype(str)\n",
    "title = ' '.join(meta['title'])\n",
    "wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='white', \n",
    "                stopwords = set(STOPWORDS), \n",
    "                min_font_size = 10).generate(title)\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()\n",
    "\n",
    "# Creating word cloud for keywords:\n",
    "string = convert_key()\n",
    "\n",
    "wordcloud_key = WordCloud(width = 800, height = 800, \n",
    "                background_color ='white', \n",
    "                stopwords = set(STOPWORDS), \n",
    "                min_font_size = 10).generate(string)\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud_key) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
